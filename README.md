#  50 Days of Machine Learning 

Welcome to the **50 Days of Machine Learning** initiative! This dynamic journey is designed to immerse you in essential machine learning concepts and techniques over the course of 50 engaging days. Each day focuses on a unique topic, empowering you to build your skills progressively.

![Machine Learning](https://miro.medium.com/v2/resize:fit:700/format:webp/1*i9FYHOGU-T4D1uvOP5zS3Q.gif)

---

## ğŸš€ Table of Contents

- [Introduction](#introduction)
- [What's Included](#whats-included)
- [Daily Topics](#daily-topics)
- [Getting Started](#getting-started)
- [Prerequisites](#prerequisites)
- [How to Contribute](#how-to-contribute)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## ğŸ“– Introduction

In a rapidly evolving technological landscape, mastering machine learning is more crucial than ever. This project aims to break down key concepts, tools, and techniques to help you navigate this exciting field with confidence. Let's dive in! ğŸŒŠ

---

## ğŸ› ï¸ What's Included

- **Daily Topics**: Covering both essential and advanced machine learning concepts.
- **Hands-On Exercises**: Written in Python ğŸ, utilizing popular libraries like Pandas, Scikit-learn, TensorFlow, and Keras.
- **Real-World Datasets**: Engage in practical data manipulation, analysis, and predictive modeling using actual datasets ğŸ—„ï¸.

---

## ğŸ“… Daily Topics

### Hereâ€™s a breakdown of what youâ€™ll learn over the 50 days:

| Day  | Topic                                                       | 
|------|-------------------------------------------------------------|
| ğŸ—“ï¸ Day 1  | **Working with CSV files**                                                                                                                                                                |
| ğŸ—“ï¸ Day 2  | **Working with JSON and SQL**                                 |
| ğŸ—“ï¸ Day 3  | **Pandas DataFrame using Web Scraping**                       |
| ğŸ—“ï¸ Day 4  | **Understanding Your Data - Descriptive Stats**              |
| ğŸ—“ï¸ Day 5  | **Univariate Analysis**                                       |
| ğŸ—“ï¸ Day 6  | **Bivariate Analysis**                                        |
| ğŸ—“ï¸ Day 7  | **Pandas Profiling**                                         |
| ğŸ—“ï¸ Day 8  | **Standardization**                                          |
| ğŸ—“ï¸ Day 9  | **Normalization**                                            |
| ğŸ—“ï¸ Day 10 | **Ordinal Encoding**                                         |
| ğŸ—“ï¸ Day 11 | **One-Hot Encoding**                                         |
| ğŸ—“ï¸ Day 12 | **Column Transformer**                                       |
| ğŸ—“ï¸ Day 13 | **Scikit-learn Pipelines**                                  |
| ğŸ—“ï¸ Day 14 | **Function Transformer**                                      |
| ğŸ—“ï¸ Day 15 | **Power Transformer**                                        |
| ğŸ—“ï¸ Day 16 | **Binning and Binarization**                                  |
| ğŸ—“ï¸ Day 17 | **Handling Mixed Variables**                                   |
| ğŸ—“ï¸ Day 18 | **Handling Date and Time**                                    |
| ğŸ—“ï¸ Day 19 | **Complete Case Analysis**                                    |
| ğŸ—“ï¸ Day 20 | **Imputing Numerical Data**                                   |
| ğŸ—“ï¸ Day 21 | **Handling Missing Categorical Data**                         |
| ğŸ—“ï¸ Day 22 | **Missing Indicator**                                         |
| ğŸ—“ï¸ Day 23 | **KNN Imputer**                                              |
| ğŸ—“ï¸ Day 24 | **Iterative Imputer**                                        |
| ğŸ—“ï¸ Day 25 | **Outlier Removal using Z-Score**                             |
| ğŸ—“ï¸ Day 26 | **Outlier Removal using IQR Method**                          |
| ğŸ—“ï¸ Day 27 | **Outlier Detection using Percentiles**                       |
| ğŸ—“ï¸ Day 28 | **Feature Construction and Feature Splitting**               |
| ğŸ—“ï¸ Day 29 | **PCA (Principal Component Analysis)**                       |
| ğŸ—“ï¸ Day 30 | **Simple Linear Regression**                                   |
| ğŸ—“ï¸ Day 31 | **Regression Metrics**                                        |
| ğŸ—“ï¸ Day 32 | **Multiple Linear Regression**                                 |
| ğŸ—“ï¸ Day 33 | **Gradient Descent**                                        |
| ğŸ—“ï¸ Day 34 | **Types of Gradient Descent**                                 |
| ğŸ—“ï¸ Day 35 | **Polynomial Regression**                                     |
| ğŸ—“ï¸ Day 36 | **Regularized Linear Models**                                  |
| ğŸ—“ï¸ Day 37 | **Lasso Regression**                                         |
| ğŸ—“ï¸ Day 38 | **ElasticNet Regression**                                     |
| ğŸ—“ï¸ Day 39 | **Logistic Regression**                                       |
| ğŸ—“ï¸ Day 40 | **Classification Metrics**                                    |
| ğŸ—“ï¸ Day 41 | **Logistic Regression (continued)**                           |
| ğŸ—“ï¸ Day 42 | **Random Forest**                                            |
| ğŸ—“ï¸ Day 43 | **AdaBoost**                                                 |
| ğŸ—“ï¸ Day 44 | **Stacking and Blending**                                     |

---

## ğŸš€ Getting Started

To kick off your learning adventure, clone the repository to your local machine:

```bash
git clone https://github.com/AdilShamim8/50-Days-of-Machine-Learning.git
cd 50-Days-of-Machine-Learning
```

Every day's folder contains a Jupyter notebook or Python script along with exciting exercises designed to reinforce the topics covered.

![GitHub](https://user-images.githubusercontent.com/72607219/136659535-256bf925-bc31-487b-91cb-2e80788fd167.png)

---

## ğŸ’» Prerequisites

Before diving in, make sure you have the following set up:

- **Python**: Version 3.6 or later ğŸ
- **Jupyter Notebook** or another IDE of your choice ğŸ’»
- **Required Libraries**: Install them via the `requirements.txt` file:

```bash
pip install -r requirements.txt
```

---

## ğŸ¤ How to Contribute

Your contributions are immensely valuable! If you're interested in enhancing this repository or adding new resources, please fork the repository, make your changes, and submit a pull request. Ensure to maintain the code style and include comments where necessary. ğŸŒ

![Fork This Repository](https://user-images.githubusercontent.com/72607219/136659791-74084e6b-b297-4b81-bbc8-f7e075210021.png)

---

## ğŸ“œ License

This project is licensed under the MIT License. For more details, please refer to the [LICENSE](LICENSE) file. ğŸ“„

---

## ğŸ™ Acknowledgments

A special shoutout to all the incredible resources and communities that inspire this journey! ğŸŒŸ 

- **Machine Learning Community**: Your collective knowledge and passion drive innovation in this field. Thank you for sharing your insights! ğŸ¤

- **Online Resources**: Websites like [Kaggle](https://www.kaggle.com), [Coursera](https://www.coursera.org), and [Towards Data Science](https://towardsdatascience.com) have been invaluable for learning and practical experience. ğŸ“š

- **Libraries and Frameworks**: A big thank you to the creators of essential libraries like [Pandas](https://pandas.pydata.org), [NumPy](https://numpy.org), [Scikit-learn](https://scikit-learn.org), [TensorFlow](https://www.tensorflow.org), and [Keras](https://keras.io) for your hard work and dedication that makes machine learning more accessible! ğŸ’»

- **Inspiration**: Finally, to all the educators, authors, and content creators who provide tutorials, articles, and videos. Your commitment to teaching is what fuels progress in this exciting field! ğŸ‰

![Thank You Community](https://media.giphy.com/media/xT9IgHihLpLMUcFdiw/giphy.gif)

Together, let's continue to innovate and explore the endless possibilities of machine learning! ğŸš€âœ¨


